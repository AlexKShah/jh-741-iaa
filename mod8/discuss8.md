Module 8 - Discussion

Please click on the Discussion Forum #8 to participate in this weeks discussion.

Answer all parts of all five (5) questions on your initial post.
Question 1:

Many vendors like Google and Amazon produce consumer goods and see transformative technologies that incorporate AI.  Should companies that conduct research, then develop and sell AI products be held to different legal, safety, liability, and ethical standards than those who don’t?  Justify your answer either way?   Also, should these same companies have specific policies that address Human-AI collaboration and accountability practices?  

Question 2:  

A growing number of researchers have begun working on the question of how to mitigate the catastrophic risks of transformative artificial intelligence, including what policy states they should adopt.  As the automotive industry starts to develop self-driving cars with such features as voice translation incorporated, who is the responsible party(s) to ensure the correct safety features and validation of the algorithms being used gets integrated into the cars?    Would that be enforced similarly to the Risk Management Framework (RMF) or Cyber Maturity Model Certification (CMMC) processes being used for placing IT systems onto a network?   Lastly,  should there be federal oversight of all AI/autonomous systems regardless of what purpose or function they serve?   

Background:

Google and Microsoft have both released their latest Artificial Intelligence (AI) invention open the world, Bard and ChatGPT respectfully.  These devices are said to increase the popular chatbot technology to generate coherent, human-like pieces of writing.  This is just the beginning, as AI coupled with ML will provide many advantages as well as concerns and challenges.

Read the attached pdf and conduct some open research, then answer the below questions.

pdf here ChatGPT_MainText.pdf

 

Question 3:

What is ChatGPT? How does it work?

Question 4:

What are the impacts (positive or negative) of ChatGPT?  Support your reasoning.

Question 5:

What are the ethical/academic concerns of ChatGPT?   Support your reasoning.

Question 6:

Review the attached "Gen AI challenges" pdf under the Generative (Gen) AI section of Module 8 and answer this question;

Under Section II. Cybersecurity Substantial Areas of Risks with Generative AI & Chat GPT", pick 2 key concerns and explain whether you agree or disagree with the author?   Justify your reasoning/explanation.  don't just say yes or no 

 

Please justify your answers and sight all sources.     

please provide your initial post by next Wed @ 1159pm EST.  this gives enough time for your classmates to respond. 

Initial posts and at least one substantive post to another classmate are due Sun, 23 MAR @ 1159pm EST.  

 

best, jason

 

Question 1

I think makers of AI systems should be held to similar standards as other products or areas with comparable impact. From the Accountability of AI paper, they show that explanation systems that work independently from AI systems are possible, and under the framework of the US legal system, explainability is beneficial and necessary to hold AI decision making to similar standards as human made decisions. [1] I like how this paper identifies the issues in governing AI around moral and ethical ambiguity and frames the arguments around the code of (US) law. Explainability is especially important as bias in AI systems can effect real lives from loan programs to medical treatment as part of human AI systems, so like any other system, it should be scrutinized to the same extent and the providers held accountable. What this could mean for regulating AI is requiring a level of accountability by makers of AI systems, so that their products are capable of having its decisions explained. The liability would be placed on the makers of AI products, and the users who allow decisions made by AI systems to be passed along in ways that can cause some sort of harm. For example how would we find fault in an autonomous vehicle if the makers of the AI systems weren't expected to be held to similar accountability as other auto makers or software companies? The system should be able to replay its sensor read outs and decision making in order for a court to find fault in an accident and other daily life scenarios.

---

Question 2

AI systems being used for autonomous and critical systems should definitely be accountable from the top down. Companies making AI products will only focus on risk as far as they are required to and avoid responsibility as much as possible, so holding a system to frameworks like RMF where continuous evaluation helps mitigate risk as they are uncovered and CMMC where 3rd party assessments and requirements help to mitigate risks ahead of inference, is important to predicting and preventing some risks that could effect consumers of the products. But I also think that trying to prevent risk and dealing with the actual effects of these systems and failures should be two different requirements. Companies should be tasked with following risk management frameworks and other regulations as well as have the direct effects of their products litigated and ruled on in courts. I think systems that have a broad reach should be regulated with strong oversight like in the US the federal government should have oversight of AI products like autonomous cars that can cross state lines, or medical decisions that are made from a California company on a resident of New York. 

---

Question 3

ChatGPT is a generative pretrained transformer chat model trained on tons of data by OpenAI and refined into an instruct model by fine tuning. This enables it to generate natural language conversationally and provide long coherent responses to user prompts.

---

Question 4

The impacts of ChatGPT are broad and not fully understood yet, as it is still quite new. But there are already real impacts. Generating text and images that are hard to distinguish from real people/reality can have profound disinformation effects. But it also enables people without certain skills to be able to quickly accomplish tasks or get the broad strokes to understanding concepts. That doesn't make it accurate or reliable however, as modern LLM models suffer from hallucinations or confidently make inaccurate statements or cite fake sources. This can lead people to the wrong information or give them the wrong ideas. For example, recent research found that even with live search, AI models answered incorrectly 60% of the time when trying to attribute the correct source to a quoted news article. [2]

---

Question 5

ChatGPT can be used to aggregate data and ideas, draw conclusions, and prepare drafts for essays and research papers subject to the same concerns as casual use like inaccuracy and bias, but with added scholarly/ethical issues like copyright and citations. With a model trained on huge amounts of data (including copyrighted content), the model could output text verbatim from a real source without attribution or invent titles and sources to back up its claims that don't exist. The New Academic Reality article talks about a need to establish ethical and transparency guidelines in academia in order to prevent eroding public trust in science and setting bad precedence.

---

Question 6

I have mentioned misinformation/manipulation and hallucinations and I think those are the two biggest challenges in LLMs. Even if you set aside the malicious use of LLMs, there are examples like the paper mentions where a lawyer cited cases that didn't exist because ChatGPT fabricated supporting sources to bolster its arguments. Most people don't go back and check the sources or question a statement of fact when presented with a convincing argument, even this case in a professional legal setting. So hallucinations and misinformation from ChatGPT itself are a broadly reaching problem. Then malicious actors can use the speed and capabilities of generating novel text to spread misinformation or propaganda faster to a huge audience online without them realizing it. These two areas overlap with some of the other concerns like deep fakes and phishing where these model's outputs are good enough now to convince a majority of people on first glance. It doesn't even have to be that involved for people to believe an AI voice to text video about something untrue. The Internet is getting flooded by AI generated content that is hard to separate from human generated content. And these outputs, whether manipulated or unintentionally incorrect, are being scraped and fed back in to create new generations of LLMs, further muddying the waters.

[1] Doshi-Velez, Finale, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O'Brien, Stuart Schieber, James Waldo, David Weinberger, and Alexandra Wood. "Accountability of AI under the law: The role of explanation." arXiv preprint arXiv:1711.01134 (2017).

[2] https://arstechnica.com/ai/2025/03/ai-search-engines-give-incorrect-answers-at-an-alarming-60-rate-study-says/

[3] Pasupuleti, Rajesh. "Cyber Security Issues and Challenges Related to Generative AI and ChatGPT."