Question 1

The lecture video describes limited value hiding email addresses from spammers and I would have to agree. When using services that create email aliases, the new email address is only good while undiscovered. Spammers can target domains and send email to possible combinations at that domain, so the address will be discovered eventually. And it's likely to be discovered quickly when in use, so the benefit to aliases are short lived. Spammers prioritize fresh email addresses, buy them cheap, and quickly begin to send them spam, making these addresses just another target for spammers. If you're trying to obfuscate your existing address, common methods like "name [at] domain [dot] com" are well understood and can be scraped from social media and personal websites, and email addresses are detectable when saved as an image through OCR. Since the obfuscation or round about ways of avoiding giving your email address out don't really work, I would agree to prioritize preventing spam through filters and other protection services. If spam is quickly and inevitably going to target an address, it would be better to spend resources cutting back what makes it through to your inbox.

---

Question 2

There aren't many avenues for communication I can think of that spammers haven't already targeted. Social media platforms, messaging apps, and most places online have already been inundated with spam. Though things required to use in your day to day like at work such as enterprise intranet/internal tools come to mind. In something like a knowledgebase, malicious actors with a means into the company or supplier network could insert a particular product as a solution that might go unnoticed. There's definitely a big overlap between regular advertising and spam in my mind and I wouldn't be surprised if corporations started selling out their employees like a company slack with partnered messages. Companies aleady offer stuff like gift cards or coupons as incentives so for example they might have metrics tied to spammy rewards. I can imagine a slack bot telling me about work partnered deals. Or there could be "sponsored tips" in a knowledgebase. You might see brand names when you tap your badge to get into work or a brand's name peppering the break room. And importantly these are avenues where the person being spammed just has to take it. If the employee tried to filter the ads in these products they may be violating some terms of their employment, which might be technically feasible but would be impractical risks that result in reprimand or termination. Companies could be offered cheaper solutions to show ads to their employees and they would likely take it and the employees would just have to suffer. Though I would hope there are some worker protections to prevent this type of thing, or at the least we have laws about cruel and unusual punishment.

---

Question 3

There are patterns detectable via machine learning that discern the difference between normal communication and coded or spammy language. Some of the differences are easier to detect like the language and vocabulary, and types of requests used in spam, while obfuscated text like leet speak examples in the lectures are more difficult for machines to parse. I think in those instances, there could be methods of visual based techniques that can look at the approximate shapes or common substitutions for characters to make up words. And there are also meta analysis components to machine learning that can inform the decision to flag a message as spam like patterns across larger volumes of messages where machine learning and other types of regressions can be used to detect malicious patterns.

--

Question 4

While some models can be resource intensive and heavy to run, and even more so to train, there could be benefits to using machine learning models in more critical accounts like for children's accounts. In most cases you would use layers of protection, so the block lists, word filter lists, and more advanced filters can be put in place ahead of a machine learnin model to cut back on the number of messages you need to perform inference on, cutting down on time and resource use. Training machine learning models definitely benefits from scale like how millions of users marking messages as spam gives strong signals to the models that Gmail and other popular products use to make spam filtering better. But to scale the resources needed for machine learning to receive updates from each user's flagged spam messages would be more difficult than bayesian detection methods. In order to do that the model would have to be optimized for training, have a more limited number of parameters to bring training time down and make moving it to inference engines quicker, or be shardable or updateable so that the many servers performing inference using the trained models would be able to make accurate classifications. If there were larger models that were updated more infrequently, they might still be useful for curtailing the obvious, but then quicker adapting spam patterns might be missed.